{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spam_classifier.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdoLeta/Spam_classifier/blob/main/Spam_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cn-XdGhYFe2A"
      },
      "source": [
        "# Importing the relevant libraries\n",
        "import tqdm\n",
        "from tensorflow.python.keras.metrics import Metric\n",
        "import tensorflow\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Embedding, LSTM, Dropout, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.utils import to_categorical\n",
        "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time\n",
        "import numpy as np\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tRTlYCWnxfa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ddfd87-4fbe-4b19-8ba1-4aa70588855e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oJMaoNrFnzM"
      },
      "source": [
        "SEQUENCE_LENGTH = 100 # the length of all sequences (number of words per sample)\n",
        "EMBEDDING_SIZE = 100  # Using 100-Dimensional GloVe embedding vectors\n",
        "TEST_SIZE = 0.25 # ratio of testing set\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 20 # number of epochs\n",
        "\n",
        "# to convert labels to integers and vice-versa\n",
        "label2int = {\"ham\": 0, \"spam\": 1}\n",
        "int2label = {0: \"ham\", 1: \"spam\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BaJuUaOqHRhq"
      },
      "source": [
        "def load_data():\n",
        "    \"\"\"\n",
        "    Loads SMS Spam Collection dataset\n",
        "    \"\"\"\n",
        "    texts, labels = [], []\n",
        "    with open(\"spam_or_not_spam.csv\") as f: ##Change\n",
        "        for line in f:\n",
        "            split = line.split()\n",
        "            labels.append(split[0].strip())\n",
        "            texts.append(' '.join(split[1:]).strip())\n",
        "    return texts, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hckQ03snmgzY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "6b92304b-b6b1-4009-82e9-caa0da7de0a0"
      },
      "source": [
        "X,y=load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-b5a0cfbff08d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-ef3fcb497136>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[1;32m      5\u001b[0m     \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spam_or_not_spam.csv\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m##Change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'spam_or_not_spam.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0O6ocBmmlx0"
      },
      "source": [
        "X[2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKahKWagf7is"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brKF4gQKf2zR"
      },
      "source": [
        "data=pd.read_csv(\"spam_or_not_spam.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U11L72ebHb4R"
      },
      "source": [
        "X=data[\"email\"].apply(str)\n",
        "y=data[\"label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lg8v4o7Jfuo7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnWuMV5kHkv4"
      },
      "source": [
        "# Text tokenization\n",
        "# vectorizing text, turning each text into sequence of integers\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X)\n",
        "# convert to sequence of integers\n",
        "X = tokenizer.texts_to_sequences(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNk6rNRDJmKI"
      },
      "source": [
        "# convert to numpy arrays\n",
        "# X = np.array(X)\n",
        "# y = np.array(y)\n",
        "# pad sequences at the beginning of each sequence with 0's\n",
        "# for example if SEQUENCE_LENGTH=4:\n",
        "# [[5, 3, 2], [5, 1, 2, 3], [3, 4]]\n",
        "# will be transformed to:\n",
        "# [[0, 5, 3, 2], [5, 1, 2, 3], [0, 0, 3, 4]]\n",
        "X = pad_sequences(X, maxlen=SEQUENCE_LENGTH)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Phdg1IYJxqg"
      },
      "source": [
        "# One Hot encoding labels\n",
        "# [spam, ham, spam, ham, ham] will be converted to:\n",
        "# [1, 0, 1, 0, 1] and then to:\n",
        "# [[0, 1], [1, 0], [0, 1], [1, 0], [0, 1]]\n",
        "\n",
        "y = to_categorical(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpwnrZAXi3GM"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve-9CHeDKOjf"
      },
      "source": [
        "In [7]: print(y[0])\n",
        "[1.0, 0.0]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xyqL81LqKolx"
      },
      "source": [
        "# split and shuffle\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmRokNZrhRPt"
      },
      "source": [
        "!unzip glove6b100dtxt.zip "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOYYBxBZK6mh"
      },
      "source": [
        "def get_embedding_vectors(tokenizer, dim=100):\n",
        "    embedding_index = {}\n",
        "    with open(f\"/content/glove.6B.100d.txt\", encoding='utf8') as f:\n",
        "        for line in tqdm.tqdm(f, \"Reading GloVe\"):\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vectors = np.asarray(values[1:], dtype='float32')\n",
        "            embedding_index[word] = vectors\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    embedding_matrix = np.zeros((len(word_index)+1, dim))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            # words not found will be 0s\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "            \n",
        "    return embedding_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prigKhU2ilR2"
      },
      "source": [
        "embedding_matrix = get_embedding_vectors(tokenizer)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSwtuR62L7d0"
      },
      "source": [
        "# Building the model\n",
        "def get_model(tokenizer, lstm_units):\n",
        "    \"\"\"\n",
        "    Constructs the model,\n",
        "    Embedding vectors => LSTM => 2 output Fully-Connected neurons with softmax activation\n",
        "    \"\"\"\n",
        "    # get the GloVe embedding vectors\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(len(tokenizer.word_index)+1,\n",
        "              EMBEDDING_SIZE,\n",
        "              weights=[embedding_matrix],\n",
        "              trainable=False,\n",
        "              input_length=SEQUENCE_LENGTH))\n",
        "\n",
        "    model.add(LSTM(lstm_units, recurrent_dropout=0.2))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(2, activation=\"softmax\"))\n",
        "    # compile as rmsprop optimizer\n",
        "    # aswell as with recall metric\n",
        "    model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\",\n",
        "                  metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPaciK66iaIU"
      },
      "source": [
        "import tensorflow as tf\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD6-iDhtiRHv"
      },
      "source": [
        "# constructs the model with 128 LSTM units\n",
        "model = get_model(tokenizer=tokenizer, lstm_units=128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll8KXsDPMRtY"
      },
      "source": [
        "# initialize our ModelCheckpoint and TensorBoard callbacks\n",
        "# model checkpoint for saving best weights\n",
        "model_checkpoint = ModelCheckpoint(\"results/spam_classifier_{val_loss:.2f}\", save_best_only=True,\n",
        "                                    verbose=1)\n",
        "# for better visualization\n",
        "tensorboard = TensorBoard(f\"logs/spam_classifier_{time.time()}\")\n",
        "# print our data shapes\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_train.shape:\", y_train.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)\n",
        "# # train the model\n",
        "# model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "#           batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "#           callbacks=[tensorboard, model_checkpoint],\n",
        "#           verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qfGZwQtiznr"
      },
      "source": [
        "# train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          batch_size=BATCH_SIZE, epochs=EPOCHS,\n",
        "          callbacks=[tensorboard, model_checkpoint],\n",
        "          verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlu8EOCKMfao"
      },
      "source": [
        "Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKwF_MQHMSHV"
      },
      "source": [
        "# get the loss and metrics\n",
        "result = model.evaluate(X_test, y_test)\n",
        "# extract those\n",
        "loss = result[0]\n",
        "accuracy = result[1]\n",
        "precision = result[2]\n",
        "recall = result[3]\n",
        "\n",
        "print(f\"[+] Accuracy: {accuracy*100:.2f}%\")\n",
        "print(f\"[+] Precision:   {precision*100:.2f}%\")\n",
        "print(f\"[+] Recall:   {recall*100:.2f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp34RHWjMSLq"
      },
      "source": [
        "def get_predictions(text):\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    # pad the sequence\n",
        "    sequence = pad_sequences(sequence, maxlen=SEQUENCE_LENGTH)\n",
        "    # get the prediction\n",
        "    prediction = model.predict_classes(sequence).flatten()\n",
        "    # one-hot encoded vector, revert using np.argmax\n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmCVxLe5MSQI"
      },
      "source": [
        "text = \"Congratulations! you have won 100,000$ this week, click here to claim fast\"\n",
        "print(get_predictions(text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVK_aAaFMSVS"
      },
      "source": [
        "text = \"Hello\"\n",
        "print(get_predictions(data.iloc[3,0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uqnea6wMxgl"
      },
      "source": [
        "from flask import Flask, render_template, request"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_c6ofT6MxmY"
      },
      "source": [
        "import pickle\n",
        "with open('classifier.pkl', 'wb') as file:\n",
        " pickle.dump(classifier, file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2w7U22KAazV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elJczuSlMxsA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fVh0JtHEMxx5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlnH4_WeMx26"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9iLqJ3FMx8B"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}